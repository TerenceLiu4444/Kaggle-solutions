{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>category_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Кокоби(кокон для сна)</td>\n",
       "      <td>Кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Стойка для Одежды</td>\n",
       "      <td>Стойка для одежды, под вешалки. С бутика.</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Philips bluray</td>\n",
       "      <td>В хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02996f1dd2ea</td>\n",
       "      <td>bf5cccea572d</td>\n",
       "      <td>Татарстан</td>\n",
       "      <td>Набережные Челны</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Автокресло</td>\n",
       "      <td>Продам кресло от0-25кг</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>286</td>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7c90be56d2ab</td>\n",
       "      <td>ef50846afc0b</td>\n",
       "      <td>Волгоградская область</td>\n",
       "      <td>Волгоград</td>\n",
       "      <td>Автомобили</td>\n",
       "      <td>ВАЗ 2110, 2003</td>\n",
       "      <td>Все вопросы по телефону.</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id       user_id                 region              city  \\\n",
       "0  b912c3c6a6ad  e00f8ff2eaf9   Свердловская область      Екатеринбург   \n",
       "1  2dac0150717d  39aeb48f0017      Самарская область            Самара   \n",
       "2  ba83aefab5dc  91e2f88dd6e3     Ростовская область    Ростов-на-Дону   \n",
       "3  02996f1dd2ea  bf5cccea572d              Татарстан  Набережные Челны   \n",
       "4  7c90be56d2ab  ef50846afc0b  Волгоградская область         Волгоград   \n",
       "\n",
       "                category_name                  title  \\\n",
       "0  Товары для детей и игрушки  Кокоби(кокон для сна)   \n",
       "1           Мебель и интерьер      Стойка для Одежды   \n",
       "2               Аудио и видео         Philips bluray   \n",
       "3  Товары для детей и игрушки             Автокресло   \n",
       "4                  Автомобили         ВАЗ 2110, 2003   \n",
       "\n",
       "                                         description    price  \\\n",
       "0  Кокон для сна малыша,пользовались меньше месяц...    400.0   \n",
       "1          Стойка для одежды, под вешалки. С бутика.   3000.0   \n",
       "2  В хорошем состоянии, домашний кинотеатр с blu ...   4000.0   \n",
       "3                             Продам кресло от0-25кг   2200.0   \n",
       "4                           Все вопросы по телефону.  40000.0   \n",
       "\n",
       "   item_seq_number activation_date user_type  \n",
       "0                2      2017-03-28   Private  \n",
       "1               19      2017-03-26   Private  \n",
       "2                9      2017-03-20   Private  \n",
       "3              286      2017-03-25   Company  \n",
       "4                3      2017-03-16   Private  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_cols = ['item_id', 'user_id', 'city', 'region', 'category_name',\n",
    "             'price','title','description', 'item_seq_number',\n",
    "             'activation_date', 'user_type']\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', usecols=used_cols)\n",
    "train_active = pd.read_csv('../input/train_active.csv', usecols=used_cols)\n",
    "test = pd.read_csv('../input/test.csv', usecols=used_cols)\n",
    "test_active = pd.read_csv('../input/test_active.csv', usecols=used_cols)\n",
    "\n",
    "train_periods = pd.read_csv('../input/periods_train.csv', \n",
    "                            parse_dates=['activation_date', 'date_from', 'date_to'])\n",
    "test_periods = pd.read_csv('../input/periods_test.csv', \n",
    "                           parse_dates=['activation_date', 'date_from', 'date_to'])\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['title_len'] = df['title'].fillna('').apply(lambda x:len(x))\n",
    "    df['description_len'] = df['description'].fillna('').apply(lambda x:len(x))\n",
    "    df['price_log'] = df['price'].apply(lambda x: np.log(x+0.01))\n",
    "    return df[['item_id', 'user_id', 'city', 'region', 'category_name',\n",
    "               'price_log','title_len','description_len','item_seq_number',\n",
    "             'activation_date', 'user_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples = pd.concat([\n",
    "    process_df(train),\n",
    "    process_df(train_active),\n",
    "    process_df(test),\n",
    "    process_df(test_active)\n",
    "]).reset_index(drop=True)\n",
    "all_samples.drop_duplicates(['item_id'], inplace=True)\n",
    "\n",
    "del train_active\n",
    "del test_active\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_count( df, group_cols, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name='{}count'.format('_'.join(group_cols))  \n",
    "    if show_agg:\n",
    "        print( \"\\nAggregating by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).\\\n",
    "            to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    \n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "##  Below a function is written to extract unique count feature from different cols\n",
    "def do_countuniq( df, group_cols, counted, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_countuniq'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"\\nCounting unqiue \", counted, \" by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    \n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "### Below a function is written to extract mean feature  from different cols\n",
    "def do_mean( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_mean'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"\\nCalculating mean of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    \n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_var'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"\\nCalculating variance of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples['item_seq_number'] = all_samples.item_seq_number.fillna(-1)\\\n",
    ".apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples['city'] = all_samples.city.fillna('NA')\n",
    "all_samples['user_type'] = all_samples.user_type.fillna('NA')\n",
    "all_samples['activation_date'] = all_samples.activation_date.fillna('NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting unqiue  item_seq_number  by  ['city'] ... and saved in city_by_item_seq_number_countuniq\n",
      "\n",
      "Counting unqiue  user_id  by  ['city'] ... and saved in city_by_user_id_countuniq\n",
      "\n",
      "Counting unqiue  category_name  by  ['city'] ... and saved in city_by_category_name_countuniq\n",
      "\n",
      "Aggregating by  ['city'] ... and saved in citycount\n",
      "\n",
      "Calculating mean of  title_len  by  ['city'] ... and saved in city_by_title_len_mean\n",
      "\n",
      "Calculating mean of  price_log  by  ['city'] ... and saved in city_by_price_log_mean\n",
      "\n",
      "Calculating mean of  description_len  by  ['city'] ... and saved in city_by_description_len_mean\n",
      "\n",
      "Calculating variance of  title_len  by  ['city'] ... and saved in city_by_title_len_var\n",
      "\n",
      "Calculating variance of  price_log  by  ['city'] ... and saved in city_by_price_log_var\n",
      "\n",
      "Calculating variance of  description_len  by  ['city'] ... and saved in city_by_description_len_var\n",
      "\n",
      "Counting unqiue  item_seq_number  by  ['activation_date'] ... and saved in activation_date_by_item_seq_number_countuniq\n",
      "\n",
      "Counting unqiue  user_id  by  ['activation_date'] ... and saved in activation_date_by_user_id_countuniq\n",
      "\n",
      "Counting unqiue  category_name  by  ['activation_date'] ... and saved in activation_date_by_category_name_countuniq\n",
      "\n",
      "Aggregating by  ['activation_date'] ... and saved in activation_datecount\n",
      "\n",
      "Calculating mean of  title_len  by  ['activation_date'] ... and saved in activation_date_by_title_len_mean\n",
      "\n",
      "Calculating mean of  price_log  by  ['activation_date'] ... and saved in activation_date_by_price_log_mean\n",
      "\n",
      "Calculating mean of  description_len  by  ['activation_date'] ... and saved in activation_date_by_description_len_mean\n",
      "\n",
      "Calculating variance of  title_len  by  ['activation_date'] ... and saved in activation_date_by_title_len_var\n",
      "\n",
      "Calculating variance of  price_log  by  ['activation_date'] ... and saved in activation_date_by_price_log_var\n",
      "\n",
      "Calculating variance of  description_len  by  ['activation_date'] ... and saved in activation_date_by_description_len_var\n",
      "\n",
      "Counting unqiue  item_seq_number  by  ['user_type'] ... and saved in user_type_by_item_seq_number_countuniq\n",
      "\n",
      "Counting unqiue  user_id  by  ['user_type'] ... and saved in user_type_by_user_id_countuniq\n",
      "\n",
      "Counting unqiue  category_name  by  ['user_type'] ... and saved in user_type_by_category_name_countuniq\n",
      "\n",
      "Aggregating by  ['user_type'] ... and saved in user_typecount\n",
      "\n",
      "Calculating mean of  title_len  by  ['user_type'] ... and saved in user_type_by_title_len_mean\n",
      "\n",
      "Calculating mean of  price_log  by  ['user_type'] ... and saved in user_type_by_price_log_mean\n",
      "\n",
      "Calculating mean of  description_len  by  ['user_type'] ... and saved in user_type_by_description_len_mean\n",
      "\n",
      "Calculating variance of  title_len  by  ['user_type'] ... and saved in user_type_by_title_len_var\n",
      "\n",
      "Calculating variance of  price_log  by  ['user_type'] ... and saved in user_type_by_price_log_var\n",
      "\n",
      "Calculating variance of  description_len  by  ['user_type'] ... and saved in user_type_by_description_len_var\n"
     ]
    }
   ],
   "source": [
    "for gpby in ['city','activation_date','user_type']:\n",
    "    all_samples = do_countuniq(all_samples, [gpby],'item_seq_number')\n",
    "    all_samples = do_countuniq(all_samples, [gpby],'user_id')\n",
    "    all_samples = do_countuniq(all_samples, [gpby],'category_name')\n",
    "    all_samples = do_count(all_samples, [gpby])\n",
    "    all_samples = do_mean(all_samples, [gpby], 'title_len')\n",
    "    all_samples = do_mean(all_samples, [gpby], 'price_log')\n",
    "    all_samples = do_mean(all_samples, [gpby], 'description_len')\n",
    "    all_samples = do_var(all_samples, [gpby], 'title_len')\n",
    "    all_samples = do_var(all_samples, [gpby], 'price_log')\n",
    "    all_samples = do_var(all_samples, [gpby], 'description_len')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting unqiue  user_id  by  ['item_seq_number'] ... and saved in item_seq_number_by_user_id_countuniq\n"
     ]
    }
   ],
   "source": [
    "gpby = 'item_seq_number'\n",
    "all_samples = do_countuniq(all_samples, [gpby],'user_id')\n",
    "all_samples = do_countuniq(all_samples, [gpby],'category_name')\n",
    "all_samples = do_countuniq(all_samples, [gpby],'city')\n",
    "all_samples = do_count(all_samples, [gpby])\n",
    "all_samples = do_mean(all_samples, [gpby], 'title_len')\n",
    "all_samples = do_mean(all_samples, [gpby], 'price_log')\n",
    "all_samples = do_mean(all_samples, [gpby], 'description_len')\n",
    "all_samples = do_var(all_samples, [gpby], 'title_len')\n",
    "all_samples = do_var(all_samples, [gpby], 'price_log')\n",
    "all_samples = do_var(all_samples, [gpby], 'description_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get city info\n",
    "df_city = all_samples[[x for x in all_samples.columns if 'city' in x[:5]]]\n",
    "df_city.drop_duplicates(['city'],inplace=True)\n",
    "df_city.to_csv('city_features.csv', index=False)\n",
    "del df_city; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activate\n",
    "df_actdate = all_samples[[x for x in all_samples.columns if 'activat' in x[:10]]]\n",
    "df_actdate.drop_duplicates(['activation_date'],inplace=True)\n",
    "df_actdate.to_csv('activation_date_features.csv', index=False)\n",
    "del df_actdate; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get usertype\n",
    "df_usrtp = all_samples[[x for x in all_samples.columns if 'user_type' in x[:10]]]\n",
    "df_usrtp.drop_duplicates(['user_type'],inplace=True)\n",
    "df_usrtp.to_csv('user_type_features.csv', index=False)\n",
    "del df_usrtp; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item_seq_number\n",
    "df_itsqn = all_samples[[x for x in all_samples.columns if 'item_seq_number' in x[:20]]]\n",
    "df_itsqn.drop_duplicates(['item_seq_number'],inplace=True)\n",
    "df_itsqn.to_csv('item_seq_number_features.csv', index=False)\n",
    "del df_itsqn; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.drop(['item_seq_number','activation_date', 'user_type'],\n",
    "                axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = pd.concat([\n",
    "    train_periods,\n",
    "    test_periods\n",
    "])\n",
    "\n",
    "del train_periods\n",
    "del test_periods\n",
    "gc.collect()\n",
    "\n",
    "all_periods['date_from_isweekend'] = all_periods['date_from'].dt.dayofweek >= 5\n",
    "all_periods['date_to_isweekend'] = all_periods['date_to'].dt.dayofweek >= 5\n",
    "all_periods['activation_date_isweekend'] = all_periods['activation_date'].dt.dayofweek >= 5\n",
    "all_periods['days_up'] = all_periods['date_to'].dt.dayofyear - all_periods['date_from'].dt.dayofyear\n",
    "\n",
    "all_periods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods['date_from_isweekend'] = all_periods['date_from'].dt.dayofweek >= 5\n",
    "all_periods['date_to_isweekend'] = all_periods['date_to'].dt.dayofweek >= 5\n",
    "all_periods['activation_date_isweekend'] = all_periods['activation_date'].dt.dayofweek >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = all_periods.groupby(['item_id'])[['days_up']]\n",
    "gp2 = all_periods.groupby(['item_id'])[[\n",
    "    'date_from_isweekend','date_to_isweekend','activation_date_isweekend']]\n",
    "\n",
    "gp_df = pd.DataFrame()\n",
    "gp_df['days_up_sum'] = gp.sum()['days_up']\n",
    "gp_df['times_put_up'] = gp.count()['days_up']\n",
    "gp_df[['date_from_isweekend','date_to_isweekend','activation_date_isweekend']] = \\\n",
    "    gp2.mean()\n",
    "gp_df.reset_index(inplace=True)\n",
    "gp_df.rename(index=str, columns={'index': 'item_id'})\n",
    "\n",
    "gp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = all_periods.drop(\n",
    "    ['date_from_isweekend','date_to_isweekend','activation_date_isweekend'],\n",
    "    axis=1)\n",
    "all_periods.drop_duplicates(['item_id'], inplace=True)\n",
    "all_periods = all_periods.merge(gp_df, on='item_id', how='left')\n",
    "all_periods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = all_periods.merge(all_samples, on='item_id', how='left')\n",
    "all_periods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = all_periods.groupby(['user_id'])[\n",
    "    ['days_up_sum', \n",
    "     'times_put_up',\n",
    "     'date_from_isweekend',\n",
    "     'date_to_isweekend',\n",
    "     'activation_date_isweekend',\n",
    "     'price_log',\n",
    "     'title_len',\n",
    "     'description_len'\n",
    "     ]\n",
    "].mean().reset_index() \\\n",
    "    .rename(index=str, columns={\n",
    "        'days_up_sum': 'avg_days_up_user',\n",
    "        'times_put_up': 'avg_times_up_user',\n",
    "        'date_from_isweekend': 'avg_date_from_isweekend',\n",
    "        'date_to_isweekend': 'avg_date_to_isweekend',\n",
    "        'activation_date_isweekend': 'avg_activation_date_isweekend',\n",
    "        'price_log': 'avg_price_log',\n",
    "        'title_len': 'avg_title_len',\n",
    "        'description_len': 'avg_description_len'\n",
    "    })\n",
    "gp.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp2 = all_periods.groupby(['user_id'])[\n",
    "    ['city', \n",
    "     'region',\n",
    "     'category_name'\n",
    "     ]\n",
    "].nunique().reset_index() \\\n",
    "    .rename(index=str, columns={\n",
    "        'city': 'nunique_city',\n",
    "        'region': 'nunique_region',\n",
    "        'category_name': 'nunique_category_name'\n",
    "    })\n",
    "gp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = gp.merge(gp2,on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user_items = all_samples.groupby(['user_id'])[['item_id']].count().reset_index() \\\n",
    "    .rename(index=str, columns={\n",
    "        'item_id': 'n_user_items'\n",
    "    })\n",
    "gp = gp.merge(n_user_items, on='user_id', how='outer')\n",
    "\n",
    "gp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.to_csv('aggregated_features_new.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
